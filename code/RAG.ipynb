{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOsgguVMSA30ioORemdMucw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunshineluyao/AgenticCases/blob/main/code/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieval-Augmented Generation (RAG) is an AI architecture that enhances the capabilities of Large Language Models (LLMs) by integrating external knowledge sources into the generation process. Traditional LLMs, while powerful, are limited to the information present in their training data, which can become outdated or insufficient for specific queries. RAG addresses this limitation by retrieving relevant information from external databases or documents in real-time, ensuring that the generated responses are both accurate and up-to-date.\n",
        "\n",
        "In a typical RAG system, when a user poses a question, the model first retrieves pertinent documents or data from an external source. This retrieved information is then combined with the model's internal knowledge to generate a response that is both contextually relevant and factually accurate. This approach not only improves the quality of AI-generated content but also mitigates issues like \"hallucinations,\" where models produce plausible-sounding but incorrect information.\n",
        "\n",
        "**Example 1: Using Hugging Face's `pipeline` with DistilBERT**\n",
        "\n",
        "In this example, we utilize Hugging Face's `pipeline` for question answering, employing the `distilbert-base-uncased-distilled-squad` model. This model is a distilled version of BERT, optimized for efficiency while maintaining performance.\n",
        "\n",
        "```python\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the question-answering pipeline\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
        "\n",
        "# Define your question\n",
        "question = \"What is the main point of the article?\"\n",
        "\n",
        "# Ensure the extracted text is not empty\n",
        "if extracted_text:\n",
        "    # Prepare the input for the model\n",
        "    qa_input = {\n",
        "        'question': question,\n",
        "        'context': extracted_text\n",
        "    }\n",
        "    # Get the answer\n",
        "    answer = qa_pipeline(qa_input)\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer['answer']}\")\n",
        "else:\n",
        "    print(\"Error: No text extracted from the PDF.\")\n",
        "```\n",
        "\n",
        "In this script, the `pipeline` is initialized for question answering with the specified model. The `question` variable holds the query we want to answer, and `extracted_text` contains the content from which the answer is to be derived. The model processes the input and returns the most probable answer found within the context.\n",
        "\n",
        "**Example 2: Handling Longer Texts with Chunking**\n",
        "\n",
        "When dealing with lengthy documents, it's essential to manage the input size to fit within the model's maximum token limit. One common approach is to split the text into manageable chunks with some overlap to ensure context continuity.\n",
        "\n",
        "```python\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the question-answering pipeline\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
        "\n",
        "def chunk_text(text, max_length, overlap):\n",
        "    \"\"\"\n",
        "    Splits the text into chunks of max_length with a specified overlap.\n",
        "\n",
        "    Args:\n",
        "        text: The input text to be chunked.\n",
        "        max_length: Maximum length of each chunk.\n",
        "        overlap: Number of overlapping tokens between chunks.\n",
        "\n",
        "    Returns:\n",
        "        A list of text chunks.\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(words):\n",
        "        end = min(start + max_length, len(words))\n",
        "        chunk = ' '.join(words[start:end])\n",
        "        chunks.append(chunk)\n",
        "        start += max_length - overlap\n",
        "    return chunks\n",
        "\n",
        "# Define your question\n",
        "question = \"What is the main point of the article?\"\n",
        "\n",
        "# Parameters\n",
        "max_chunk_length = 450  # Adjust based on model's max token limit minus space for the question\n",
        "overlap_length = 50     # Number of overlapping tokens\n",
        "\n",
        "# Split the extracted text into chunks\n",
        "text_chunks = chunk_text(extracted_text, max_chunk_length, overlap_length)\n",
        "\n",
        "# Iterate over chunks and get answers\n",
        "answers = []\n",
        "for chunk in text_chunks:\n",
        "    qa_input = {\n",
        "        'question': question,\n",
        "        'context': chunk\n",
        "    }\n",
        "    answer = qa_pipeline(qa_input)\n",
        "    answers.append(answer['answer'])\n",
        "\n",
        "# Combine or select the most appropriate answer\n",
        "# For simplicity, we'll just print all answers here\n",
        "for idx, ans in enumerate(answers):\n",
        "    print(f\"Answer from chunk {idx + 1}: {ans}\")\n",
        "```\n",
        "\n",
        "In this script, the `chunk_text` function divides the `extracted_text` into smaller segments, each with a specified maximum length and overlap. This ensures that the model can process each chunk without exceeding its token limit. The script then iterates over these chunks, applies the question-answering pipeline to each, and collects the answers. Finally, it prints the answers obtained from each chunk.\n",
        "\n",
        "By employing such techniques, we can effectively handle longer texts and improve the accuracy of AI-generated responses, especially when combined with RAG architectures that provide access to external, up-to-date information.\n",
        "\n"
      ],
      "metadata": {
        "id": "EeB-RMgAl8dY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install  PyPDF2 transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2k5DODag8Yf",
        "outputId": "028d7f89-eaec-483c-c670-b384de75935e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: smolagents in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from smolagents) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from smolagents) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from smolagents) (0.20.1+cu121)\n",
            "Requirement already satisfied: requests>=2.32.3 in /usr/local/lib/python3.10/dist-packages (from smolagents) (2.32.3)\n",
            "Requirement already satisfied: rich>=13.9.4 in /usr/local/lib/python3.10/dist-packages (from smolagents) (13.9.4)\n",
            "Requirement already satisfied: pandas>=2.2.3 in /usr/local/lib/python3.10/dist-packages (from smolagents) (2.2.3)\n",
            "Requirement already satisfied: jinja2>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from smolagents) (3.1.4)\n",
            "Requirement already satisfied: pillow>=11.0.0 in /usr/local/lib/python3.10/dist-packages (from smolagents) (11.0.0)\n",
            "Requirement already satisfied: markdownify>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from smolagents) (0.14.1)\n",
            "Requirement already satisfied: gradio>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from smolagents) (5.10.0)\n",
            "Requirement already satisfied: duckduckgo-search>=6.3.7 in /usr/local/lib/python3.10/dist-packages (from smolagents) (7.2.1)\n",
            "Requirement already satisfied: python-dotenv>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from smolagents) (1.0.1)\n",
            "Requirement already satisfied: e2b-code-interpreter>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from smolagents) (1.0.3)\n",
            "Requirement already satisfied: litellm>=1.55.10 in /usr/local/lib/python3.10/dist-packages (from smolagents) (1.57.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: click>=8.1.7 in /usr/local/lib/python3.10/dist-packages (from duckduckgo-search>=6.3.7->smolagents) (8.1.7)\n",
            "Requirement already satisfied: primp>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from duckduckgo-search>=6.3.7->smolagents) (0.10.0)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from duckduckgo-search>=6.3.7->smolagents) (5.3.0)\n",
            "Requirement already satisfied: attrs>=21.3.0 in /usr/local/lib/python3.10/dist-packages (from e2b-code-interpreter>=1.0.3->smolagents) (24.3.0)\n",
            "Requirement already satisfied: e2b<2.0.0,>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from e2b-code-interpreter>=1.0.3->smolagents) (1.0.5)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from e2b-code-interpreter>=1.0.3->smolagents) (0.27.2)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=5.8.0->smolagents) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=5.8.0->smolagents) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.10/dist-packages (from gradio>=5.8.0->smolagents) (0.115.6)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio>=5.8.0->smolagents) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.5.3 in /usr/local/lib/python3.10/dist-packages (from gradio>=5.8.0->smolagents) (1.5.3)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=5.8.0->smolagents) (2.1.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=5.8.0->smolagents) (3.10.12)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=5.8.0->smolagents) (2.10.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio>=5.8.0->smolagents) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.10/dist-packages (from gradio>=5.8.0->smolagents) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio>=5.8.0->smolagents) (0.8.6)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from gradio>=5.8.0->smolagents) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=5.8.0->smolagents) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=5.8.0->smolagents) (0.41.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=5.8.0->smolagents) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio>=5.8.0->smolagents) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=5.8.0->smolagents) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=5.8.0->smolagents) (0.34.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.3->gradio>=5.8.0->smolagents) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.3->gradio>=5.8.0->smolagents) (14.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from litellm>=1.55.10->smolagents) (3.11.10)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.10/dist-packages (from litellm>=1.55.10->smolagents) (8.5.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from litellm>=1.55.10->smolagents) (4.23.0)\n",
            "Requirement already satisfied: openai>=1.55.3 in /usr/local/lib/python3.10/dist-packages (from litellm>=1.55.10->smolagents) (1.57.4)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from litellm>=1.55.10->smolagents) (0.8.0)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.9 in /usr/local/lib/python3.10/dist-packages (from markdownify>=0.14.1->smolagents) (4.12.3)\n",
            "Requirement already satisfied: six<2,>=1.15 in /usr/local/lib/python3.10/dist-packages (from markdownify>=0.14.1->smolagents) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.2.3->smolagents) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.2.3->smolagents) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.2.3->smolagents) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.3->smolagents) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.3->smolagents) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.3->smolagents) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.3->smolagents) (2024.12.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.9.4->smolagents) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.9.4->smolagents) (2.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->smolagents) (3.4.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->smolagents) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->smolagents) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=5.8.0->smolagents) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=5.8.0->smolagents) (1.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5,>=4.9->markdownify>=0.14.1->smolagents) (2.6)\n",
            "Requirement already satisfied: httpcore<2.0.0,>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from e2b<2.0.0,>=1.0.4->e2b-code-interpreter>=1.0.3->smolagents) (1.0.7)\n",
            "Requirement already satisfied: protobuf<6.0.0,>=3.20.0 in /usr/local/lib/python3.10/dist-packages (from e2b<2.0.0,>=1.0.4->e2b-code-interpreter>=1.0.3->smolagents) (4.25.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore<2.0.0,>=1.0.5->e2b<2.0.0,>=1.0.4->e2b-code-interpreter>=1.0.3->smolagents) (0.14.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.8.0->litellm>=1.55.10->smolagents) (3.21.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.55.10->smolagents) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.55.10->smolagents) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.55.10->smolagents) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->smolagents) (0.1.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.55.3->litellm>=1.55.10->smolagents) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.55.3->litellm>=1.55.10->smolagents) (0.8.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio>=5.8.0->smolagents) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio>=5.8.0->smolagents) (2.27.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=5.8.0->smolagents) (1.5.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm>=1.55.10->smolagents) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm>=1.55.10->smolagents) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm>=1.55.10->smolagents) (4.0.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm>=1.55.10->smolagents) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm>=1.55.10->smolagents) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm>=1.55.10->smolagents) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm>=1.55.10->smolagents) (1.18.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from smolagents import CodeAgent, tool\n",
        "import PyPDF2\n",
        "from google.colab import files\n",
        "from transformers import pipeline\n"
      ],
      "metadata": {
        "id": "eKjUDrz3g1JU"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "\n",
        "def extract_text_from_pdf(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file.\n",
        "\n",
        "    Args:\n",
        "        file_path: The path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "        The extracted text as a string.\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with open(file_path, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            for page_num, page in enumerate(reader.pages):\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    text += page_text\n",
        "                else:\n",
        "                    print(f\"Warning: No text extracted from page {page_num + 1}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "KcHqmx5WhHFC"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the PDF file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "file_path = list(uploaded.keys())[0]\n",
        "\n",
        "# Extract text from the PDF\n",
        "extracted_text = extract_text_from_pdf(file_path)\n",
        "\n",
        "# Output the extracted text length and a snippet\n",
        "print(f\"Extracted text length: {len(extracted_text)}\")\n",
        "print(f\"Extracted text snippet: {extracted_text[:500]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "U0WqYxcIj8xW",
        "outputId": "3bb9936d-cb93-4ac3-b844-0d1e91c2270c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d9811a66-cb35-41fb-a09e-01c931d57867\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d9811a66-cb35-41fb-a09e-01c931d57867\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving pgae191.pdf to pgae191 (4).pdf\n",
            "Extracted text length: 126772\n",
            "Extracted text snippet: The impact of generative artificial intelligence on \n",
            "socioeconomic inequalities and policy making\n",
            "Valerio Capraro \n",
            "a,*, Austin Lentsch \n",
            "b, Daron Acemoglu \n",
            "c, Selin Akgund, Aisel Akhmedovad, Ennio Bilancini \n",
            "e,  \n",
            "Jean-François Bonnefon \n",
            "f, Pablo Brañas-Garza \n",
            "g, Luigi Butera \n",
            "h, Karen M. Douglas \n",
            "j, Jim A.C. Everett \n",
            "j, Gerd Gigerenzer \n",
            "k, \n",
            "Christine Greenhowd, Daniel A. Hashimotol,m, Julianne Holt-Lunstad \n",
            "n, Jolanda Jetten \n",
            "o, Simon Johnsonp, Werner H. Kunz \n",
            "q, \n",
            "Chiara Longoni \n",
            "r, Pete Lunns, S\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the question-answering pipeline\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
        "\n",
        "# Define your question\n",
        "question = \"What is the main point of the article?\"\n",
        "\n",
        "# Ensure the extracted text is not empty\n",
        "if extracted_text:\n",
        "    # Prepare the input for the model\n",
        "    qa_input = {\n",
        "        'question': question,\n",
        "        'context': extracted_text\n",
        "    }\n",
        "    # Get the answer\n",
        "    answer = qa_pipeline(qa_input)\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer['answer']}\")\n",
        "else:\n",
        "    print(\"Error: No text extracted from the PDF.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_d9RV-6jHoN",
        "outputId": "349dd20e-541d-497e-a2e3-ebff76f3cca3"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the main point of the article?\n",
            "Answer: observing \n",
            "changes in their ethical standards\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the question-answering pipeline\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
        "\n",
        "def chunk_text(text, max_length, overlap):\n",
        "    \"\"\"\n",
        "    Splits the text into chunks of max_length with a specified overlap.\n",
        "\n",
        "    Args:\n",
        "        text: The input text to be chunked.\n",
        "        max_length: Maximum length of each chunk.\n",
        "        overlap: Number of overlapping tokens between chunks.\n",
        "\n",
        "    Returns:\n",
        "        A list of text chunks.\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(words):\n",
        "        end = min(start + max_length, len(words))\n",
        "        chunk = ' '.join(words[start:end])\n",
        "        chunks.append(chunk)\n",
        "        start += max_length - overlap\n",
        "    return chunks\n",
        "\n",
        "# Define your question\n",
        "question = \"What is the main point of the article?\"\n",
        "\n",
        "# Parameters\n",
        "max_chunk_length = 450  # Adjust based on model's max token limit minus space for the question\n",
        "overlap_length = 50     # Number of overlapping tokens\n",
        "\n",
        "# Split the extracted text into chunks\n",
        "text_chunks = chunk_text(extracted_text, max_chunk_length, overlap_length)\n",
        "\n",
        "# Iterate over chunks and get answers\n",
        "answers = []\n",
        "for chunk in text_chunks:\n",
        "    qa_input = {\n",
        "        'question': question,\n",
        "        'context': chunk\n",
        "    }\n",
        "    answer = qa_pipeline(qa_input)\n",
        "    answers.append(answer['answer'])\n",
        "\n",
        "# Combine or select the most appropriate answer\n",
        "# For simplicity, we'll just print all answers here\n",
        "for idx, ans in enumerate(answers):\n",
        "    print(f\"Answer from chunk {idx + 1}: {ans}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6msPwilWlNoQ",
        "outputId": "65c3784b-81c4-4a6d-c6a2-cadf2bb92f1d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer from chunk 1: The impact of generative artificial intelligence\n",
            "Answer from chunk 2: highlighting the role of policymaking\n",
            "Answer from chunk 3: discussing the impact of generative AI in the information domain\n",
            "Answer from chunk 4: addressing the socio - economic risks that we identify\n",
            "Answer from chunk 5: persuasive propaganda\n",
            "Answer from chunk 6: accurate news\n",
            "Answer from chunk 7: their behavior is likely to change\n",
            "Answer from chunk 8: users’ access to information\n",
            "Answer from chunk 9: specific policy recommendations\n",
            "Answer from chunk 10: optimism\n",
            "Answer from chunk 11: the impact of generative AI on information\n",
            "Answer from chunk 12: challenging the balance between accessibility and content accuracy\n",
            "Answer from chunk 13: rebuild the middle class\n",
            "Answer from chunk 14: digital skills training\n",
            "Answer from chunk 15: impacts of generative AI in the work - place\n",
            "Answer from chunk 16: audit and address biases within educational systems\n",
            "Answer from chunk 17: Analyze organizational decisions\n",
            "Answer from chunk 18: whether generative AI should be banned\n",
            "Answer from chunk 19: whether generative AI should be banned\n",
            "Answer from chunk 20: gift of time\n",
            "Answer from chunk 21: simplify medical jargon\n",
            "Answer from chunk 22: Summary of the main research directions on the impact of generative AI on education\n",
            "Answer from chunk 23: Assess improvements\n",
            "Answer from chunk 24: public trust in AI technologies in health - care\n",
            "Answer from chunk 25: likely to be a poor or even danger - ous replacement for human interaction\n",
            "Answer from chunk 26: uncertainty\n",
            "Answer from chunk 27: Summary of the main research directions on the impact of generative AI on healthcare\n",
            "Answer from chunk 28: Implement an AI-driven mobile health application in rural communities\n",
            "Answer from chunk 29: transparency re - quirements must be mandatory\n",
            "Answer from chunk 30: Labor voice and control of consumer information\n",
            "Answer from chunk 31: 08 January 2025 policy decisions\n",
            "Answer from chunk 32: actions\n",
            "Answer from chunk 33: best-case scenario\n",
            "Answer from chunk 34: do- main of information\n",
            "Answer from chunk 35: Consequences of conspiracy theories\n",
            "Answer from chunk 36: investigation\n",
            "Answer from chunk 37: Written testimony\n",
            "Answer from chunk 38: The dark side of so- cial movements\n",
            "Answer from chunk 39: an interdisciplinary re- view of the experimental evidence on how humans interact with machines\n",
            "Answer from chunk 40: Navigating the jagged technological frontier\n",
            "Answer from chunk 41: Applying AI to rebuild middle class jobs\n",
            "Answer from chunk 42: Accuracy of a generative artificial intelligence model in a complex diagnostic challenge\n",
            "Answer from chunk 43: factors for coronary heart disease and stroke: systematic review\n",
            "Answer from chunk 44: a systematic review and meta-analysis\n",
            "Answer from chunk 45: programming the algorith - mic social contract\n"
          ]
        }
      ]
    }
  ]
}